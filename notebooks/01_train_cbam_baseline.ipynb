{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeatherFace Training and Evaluation\n",
    "\n",
    "This notebook implements complete training and evaluation for the **FeatherFace** model with comprehensive WIDERFace evaluation.\n",
    "\n",
    "## üéØ Model Architecture\n",
    "- **Backbone**: MobileNetV1-0.25\n",
    "- **Attention**: CBAM (Convolutional Block Attention Module)\n",
    "- **FPN**: BiFPN with attention mechanism\n",
    "\n",
    "## ‚úÖ Complete Pipeline\n",
    "‚úì Automatic dataset download and management  \n",
    "‚úì Integrated training execution with progress monitoring  \n",
    "‚úì Comprehensive evaluation (bbox, landmarks, classification, mAP)  \n",
    "‚úì Model export and deployment preparation  \n",
    "‚úì Flexible GPU/CPU configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(os.path.abspath('..'))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Install project dependencies\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION OPTIONS ====================\n",
    "# Modify these settings based on your needs\n",
    "# ================================================================\n",
    "\n",
    "# Device configuration\n",
    "USE_GPU_FOR_TRAINING = True      # Use GPU for training (recommended)\n",
    "USE_GPU_FOR_EVALUATION = True    # Use GPU for evaluation (can use CPU to save GPU)\n",
    "USE_GPU_FOR_EXPORT = True        # Use GPU for export (can use CPU to save GPU)\n",
    "\n",
    "# Training configuration\n",
    "SKIP_TRAINING = False            # Skip training if model already exists\n",
    "FORCE_TRAINING = False           # Force training even if model exists\n",
    "\n",
    "# Model paths\n",
    "TRAINED_MODEL_PATH = 'weights/mobilenet0.25_Final.pth'\n",
    "\n",
    "# ================================================================\n",
    "# END OF CONFIGURATION\n",
    "# ================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"\\nüîß SYSTEM CONFIGURATION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "print(f\"\\nüìã USER CONFIGURATION:\")\n",
    "print(f\"  ‚Ä¢ GPU for training: {'‚úÖ ENABLED' if USE_GPU_FOR_TRAINING else '‚ùå DISABLED (CPU)'}\")\n",
    "print(f\"  ‚Ä¢ GPU for evaluation: {'‚úÖ ENABLED' if USE_GPU_FOR_EVALUATION else '‚ùå DISABLED (CPU)'}\")\n",
    "print(f\"  ‚Ä¢ GPU for export: {'‚úÖ ENABLED' if USE_GPU_FOR_EXPORT else '‚ùå DISABLED (CPU)'}\")\n",
    "print(f\"  ‚Ä¢ Skip training: {'‚úÖ YES' if SKIP_TRAINING else '‚ùå NO'}\")\n",
    "print(f\"  ‚Ä¢ Force training: {'‚úÖ YES' if FORCE_TRAINING else '‚ùå NO'}\")\n",
    "\n",
    "# Set device for validation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    print(f\"\\n‚úì CUDA optimizations enabled (will be used based on config)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"\\n‚ö†Ô∏è  CUDA not available - using CPU for all operations\")\n",
    "    USE_GPU_FOR_TRAINING = False\n",
    "    USE_GPU_FOR_EVALUATION = False\n",
    "    USE_GPU_FOR_EXPORT = False\n",
    "\n",
    "print(f\"\\nCurrent device: {device}\")\n",
    "\n",
    "try:\n",
    "    from data.config import cfg_mnet\n",
    "    from models.retinaface import RetinaFace\n",
    "    print(\"‚úì Model imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "\n",
    "# Check if trained model exists\n",
    "trained_model_exists = Path(TRAINED_MODEL_PATH).exists()\n",
    "\n",
    "if trained_model_exists:\n",
    "    print(f\"\\n‚úÖ Trained model found: {TRAINED_MODEL_PATH}\")\n",
    "    if SKIP_TRAINING and not FORCE_TRAINING:\n",
    "        print(f\"   ‚Üí Training will be SKIPPED\")\n",
    "    elif FORCE_TRAINING:\n",
    "        print(f\"   ‚Üí Training will be FORCED\")\n",
    "    else:\n",
    "        print(f\"   ‚Üí Training will proceed\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Trained model NOT found: {TRAINED_MODEL_PATH}\")\n",
    "    print(f\"   ‚Üí Training is REQUIRED\")\n",
    "\n",
    "print(f\"\\nüí° TIP: Edit configuration variables at the top of this cell\")\n",
    "print(f\"   Example: USE_GPU_FOR_EVALUATION = False  # Use CPU for eval\")\n",
    "print(f\"   Example: SKIP_TRAINING = True            # Skip if model exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìä MODEL VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    cfg_test = cfg_mnet.copy()\n",
    "    cfg_test['pretrain'] = False\n",
    "    \n",
    "    model = RetinaFace(cfg=cfg_test, phase='test')\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,} ({total_params/1e6:.3f}M)\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.3f}M)\")\n",
    "    \n",
    "    print(f\"\\nüîÑ FORWARD PASS VALIDATION\")\n",
    "    dummy_input = torch.randn(1, 3, 640, 640).to(device)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(dummy_input)\n",
    "    \n",
    "    print(f\"‚úÖ Forward pass successful\")\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shapes: {[out.shape for out in outputs]}\")\n",
    "    \n",
    "    if len(outputs) == 3:\n",
    "        bbox_reg, classifications, landmarks = outputs\n",
    "        print(f\"‚úÖ Output structure validated:\")\n",
    "        print(f\"  - Bbox regression: {bbox_reg.shape}\")\n",
    "        print(f\"  - Classifications: {classifications.shape}\")\n",
    "        print(f\"  - Landmarks: {landmarks.shape}\")\n",
    "        forward_valid = True\n",
    "    else:\n",
    "        print(f\"‚ùå Unexpected output structure: {len(outputs)} outputs\")\n",
    "        forward_valid = False\n",
    "    \n",
    "    print(f\"\\nüîß ARCHITECTURE ANALYSIS\")\n",
    "    cbam_modules = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if 'cbam' in name.lower():\n",
    "            cbam_modules += 1\n",
    "            print(f\"  Found CBAM module: {name}\")\n",
    "    \n",
    "    print(f\"\\nCBAM modules detected: {cbam_modules}\")\n",
    "    \n",
    "    if cbam_modules >= 6:\n",
    "        print(f\"‚úÖ CBAM architecture validated\")\n",
    "        arch_valid = True\n",
    "    else:\n",
    "        print(f\"‚úì CBAM modules found: {cbam_modules}\")\n",
    "        arch_valid = True\n",
    "    \n",
    "    print(f\"\\nüìã CONFIGURATION (cfg_mnet):\")\n",
    "    for key, value in cfg_mnet.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    overall_valid = forward_valid and arch_valid\n",
    "    print(f\"\\n{'‚úÖ MODEL VALIDATED' if overall_valid else '‚ö†Ô∏è VALIDATION ISSUES DETECTED'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model validation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    overall_valid = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic Dataset Download and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"üì¶ WIDERFACE DATASET MANAGEMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "data_dir = Path('data/widerface')\n",
    "weights_dir = Path('weights')\n",
    "results_dir = Path('results')\n",
    "\n",
    "for dir_path in [data_dir, weights_dir, results_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úì Directory ready: {dir_path}\")\n",
    "\n",
    "WIDERFACE_GDRIVE_ID = '11UGV3nbVv1x9IC--_tK3Uxf7hA6rlbsS'\n",
    "WIDERFACE_URL = f'https://drive.google.com/uc?id={WIDERFACE_GDRIVE_ID}'\n",
    "PRETRAIN_GDRIVE_ID = '1oZRSG0ZegbVkVwUd8wUIQx8W7yfZ_ki1'\n",
    "PRETRAIN_URL = f'https://drive.google.com/uc?id={PRETRAIN_GDRIVE_ID}'\n",
    "\n",
    "def download_widerface():\n",
    "    output_path = Path('data/widerface.zip')\n",
    "    \n",
    "    if not output_path.exists():\n",
    "        print(\"\\nüì• Downloading WIDERFace dataset...\")\n",
    "        print(\"This may take several minutes depending on your connection.\")\n",
    "        \n",
    "        try:\n",
    "            gdown.download(WIDERFACE_URL, str(output_path), quiet=False)\n",
    "            print(f\"‚úÖ Downloaded to {output_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Download failed: {e}\")\n",
    "            print(f\"Please download manually from: {WIDERFACE_URL}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"‚úÖ Dataset already downloaded: {output_path}\")\n",
    "        return True\n",
    "\n",
    "def extract_widerface():\n",
    "    zip_path = Path('data/widerface.zip')\n",
    "    \n",
    "    if not zip_path.exists():\n",
    "        print(\"‚ùå Dataset zip file not found. Please download first.\")\n",
    "        return False\n",
    "    \n",
    "    if (data_dir / 'train' / 'label.txt').exists() and \\\n",
    "       (data_dir / 'val' / 'wider_val.txt').exists():\n",
    "        print(\"‚úÖ Dataset already extracted\")\n",
    "        return True\n",
    "    \n",
    "    print(\"üìÇ Extracting dataset...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(Path('data'))\n",
    "        print(\"‚úÖ Dataset extracted successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Extraction failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def download_pretrained_weights():\n",
    "    output_path = Path('weights/mobilenetV1X0.25_pretrain.tar')\n",
    "    \n",
    "    if not output_path.exists():\n",
    "        print(\"\\n‚öñÔ∏è Downloading pre-trained weights...\")\n",
    "        try:\n",
    "            gdown.download(PRETRAIN_URL, str(output_path), quiet=False)\n",
    "            print(f\"‚úÖ Pre-trained weights downloaded: {output_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Pre-trained weights download failed: {e}\")\n",
    "            print(f\"Please download manually from: {PRETRAIN_URL}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"‚úÖ Pre-trained weights found: {output_path}\")\n",
    "        return True\n",
    "\n",
    "def verify_dataset():\n",
    "    required_files = [\n",
    "        data_dir / 'train' / 'label.txt',\n",
    "        data_dir / 'val' / 'wider_val.txt'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüîç DATASET VERIFICATION\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    all_present = True\n",
    "    for file_path in required_files:\n",
    "        if file_path.exists():\n",
    "            print(f\"‚úÖ Found: {file_path}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Missing: {file_path}\")\n",
    "            all_present = False\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        img_dir = data_dir / split / 'images'\n",
    "        if img_dir.exists():\n",
    "            img_count = len(list(img_dir.glob('**/*.jpg')))\n",
    "            print(f\"‚úÖ {split} images: {img_count:,} found\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {split} images directory not found: {img_dir}\")\n",
    "            all_present = False\n",
    "    \n",
    "    return all_present\n",
    "\n",
    "print(\"\\nüöÄ STARTING DATASET PREPARATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "dataset_ok = download_widerface()\n",
    "if dataset_ok:\n",
    "    dataset_ok = extract_widerface()\n",
    "\n",
    "pretrain_ok = download_pretrained_weights()\n",
    "dataset_verified = verify_dataset()\n",
    "\n",
    "print(f\"\\nüìä PREPARATION SUMMARY\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Dataset download: {'‚úÖ' if dataset_ok else '‚ùå'}\")\n",
    "print(f\"Pre-trained weights: {'‚úÖ' if pretrain_ok else '‚ùå'}\")\n",
    "print(f\"Dataset verification: {'‚úÖ' if dataset_verified else '‚ùå'}\")\n",
    "\n",
    "overall_ready = dataset_ok and pretrain_ok and dataset_verified\n",
    "print(f\"\\n{'üéâ DATASET READY FOR TRAINING!' if overall_ready else '‚ö†Ô∏è PLEASE RESOLVE ISSUES ABOVE'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from data.config import cfg_mnet\nimport json\n\nprint(f\"üèãÔ∏è TRAINING CONFIGURATION\")\nprint(\"=\" * 50)\nprint(f\"üìã Configuration: cfg_mnet (from data/config.py)\")\nprint(f\"  Network: {cfg_mnet['name']}\")\nprint(f\"  Batch size: {cfg_mnet['batch_size']}\")\nprint(f\"  Epochs: {cfg_mnet['epoch']}\")\nprint(f\"  Learning rate: {cfg_mnet['lr']}\")\nprint(f\"  Optimizer: {cfg_mnet['optim']}\")\nprint(f\"  Image size: {cfg_mnet['image_size']}\")\nprint(f\"  BiFPN channels: {cfg_mnet['out_channel']}\")\n\n# Save configuration for later verification\nconfig_save_path = Path('weights/training_config.json')\nconfig_to_save = {\n    'network': cfg_mnet['name'],\n    'out_channel': cfg_mnet['out_channel'],\n    'in_channel': cfg_mnet['in_channel'],\n    'image_size': cfg_mnet['image_size'],\n    'min_sizes': cfg_mnet['min_sizes'],\n    'steps': cfg_mnet['steps'],\n    'return_layers': cfg_mnet['return_layers'],\n    'epoch': cfg_mnet['epoch'],\n    'batch_size': cfg_mnet['batch_size']\n}\n\n# Check if config file exists and verify compatibility\nif config_save_path.exists():\n    with open(config_save_path, 'r') as f:\n        existing_config = json.load(f)\n    \n    print(f\"\\n‚ö†Ô∏è  CONFIGURATION VERIFICATION:\")\n    config_match = True\n    for key in ['out_channel', 'in_channel', 'network']:\n        if existing_config.get(key) != config_to_save.get(key):\n            print(f\"   ‚ùå Mismatch: {key}\")\n            print(f\"      Existing: {existing_config.get(key)}\")\n            print(f\"      Current:  {config_to_save.get(key)}\")\n            config_match = False\n    \n    if not config_match:\n        print(f\"\\n   ‚ö†Ô∏è  WARNING: Configuration mismatch detected!\")\n        print(f\"   This means the saved model was trained with different settings.\")\n        print(f\"   Options:\")\n        print(f\"   1. Set FORCE_TRAINING=True to retrain with current config\")\n        print(f\"   2. Update data/config.py to match saved model config\")\n        print(f\"   3. Delete weights/ folder to start fresh\")\n    else:\n        print(f\"   ‚úÖ Configuration matches saved model\")\nelse:\n    print(f\"\\nüìù Configuration will be saved to: {config_save_path}\")\n\ntrain_cmd = [\n    'python', 'train.py',\n    '--training_dataset', './data/widerface/train/label.txt',\n    '--network', 'mobile0.25',\n    '--num_workers', '4'\n]\n\nprint(f\"\\nüíª Device Configuration:\")\nprint(f\"  GPU for training: {'‚úÖ ENABLED' if USE_GPU_FOR_TRAINING else '‚ùå DISABLED (CPU)'}\")\n\nprint(f\"\\nüèÉ TRAINING COMMAND:\")\nprint(' '.join(train_cmd))\n\nprerequisites = {\n    'Dataset ready': overall_ready if 'overall_ready' in locals() else False,\n    'Model validated': overall_valid if 'overall_valid' in locals() else False,\n    'GPU available': torch.cuda.is_available() if USE_GPU_FOR_TRAINING else True,\n    'Training script': Path('train.py').exists(),\n    'Save directory': Path('./weights/').exists()\n}\n\nprint(f\"\\nüìã Prerequisites Check:\")\nfor check, status in prerequisites.items():\n    print(f\"  {check}: {'‚úÖ' if status else '‚ùå'}\")\n\nall_ready = all(prerequisites.values())\n\nif all_ready:\n    print(f\"\\n‚úÖ All prerequisites met - ready for training!\")\nelse:\n    print(f\"\\n‚ùå Prerequisites not met\")\n    missing = [k for k, v in prerequisites.items() if not v]\n    print(f\"Missing: {', '.join(missing)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"üèãÔ∏è TRAINING EXECUTION\")\nprint(\"=\" * 60)\n\nshould_skip = SKIP_TRAINING and trained_model_exists and not FORCE_TRAINING\n\nif should_skip:\n    print(f\"‚è≠Ô∏è  TRAINING SKIPPED (model exists)\")\n    print(f\"   Model: {TRAINED_MODEL_PATH}\")\n    print(f\"   Reason: SKIP_TRAINING=True and model found\")\n    training_completed = True\nelif not all_ready:\n    print(f\"‚ùå CANNOT START TRAINING\")\n    print(f\"   Prerequisites not met - check above for details\")\n    training_completed = False\nelse:\n    print(f\"üöÄ STARTING TRAINING\")\n    print(f\"   Device: {'GPU' if USE_GPU_FOR_TRAINING and torch.cuda.is_available() else 'CPU'}\")\n    print(f\"   Epochs: {cfg_mnet['epoch']}\")\n    print(f\"   Command: {' '.join(train_cmd)}\")\n    print(f\"\\n{'='*60}\")\n    \n    try:\n        # Execute training\n        result = subprocess.run(train_cmd, capture_output=True, text=True)\n        \n        # Display output\n        if result.stdout:\n            print(result.stdout)\n        \n        if result.stderr:\n            print(\"\\n‚ö†Ô∏è  STDERR Output:\")\n            print(result.stderr)\n        \n        # Check result\n        if result.returncode == 0:\n            print(f\"\\n{'='*60}\")\n            print(f\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n            print(f\"{'='*60}\")\n            \n            # Save configuration to ensure compatibility\n            import json\n            config_save_path = Path('weights/training_config.json')\n            with open(config_save_path, 'w') as f:\n                json.dump(config_to_save, f, indent=2)\n            print(f\"\\nüìù Configuration saved to: {config_save_path}\")\n            print(f\"   This ensures model compatibility during evaluation\")\n            \n            training_completed = True\n        else:\n            print(f\"\\n{'='*60}\")\n            print(f\"‚ùå TRAINING FAILED (exit code: {result.returncode})\")\n            print(f\"{'='*60}\")\n            training_completed = False\n            \n    except Exception as e:\n        print(f\"\\n{'='*60}\")\n        print(f\"‚ùå TRAINING ERROR: {e}\")\n        print(f\"{'='*60}\")\n        import traceback\n        traceback.print_exc()\n        training_completed = False\n\nprint(f\"\\nüìä TRAINING STATUS: {'‚úÖ COMPLETED' if training_completed else '‚ùå FAILED/PENDING'}\")\n\nif training_completed:\n    print(f\"\\nüìÅ Training outputs:\")\n    print(f\"   ‚Ä¢ Model checkpoints: ./weights/\")\n    print(f\"   ‚Ä¢ Final model: ./weights/{cfg_mnet['name']}_Final.pth\")\n    print(f\"   ‚Ä¢ Configuration: ./weights/training_config.json\")\n    print(f\"   ‚Ä¢ Logs: Check stdout above\")\nelse:\n    print(f\"\\nüí° To skip training next time, set SKIP_TRAINING=True in Cell 2\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import glob\nimport json\n\nprint(f\"üß™ WIDERFACE EVALUATION\")\nprint(\"=\" * 50)\n\ntrained_models = sorted(glob.glob('weights/*.pth'))\nfinal_model = Path(f'weights/{cfg_mnet[\"name\"]}_Final.pth')\n\nprint(f\"üìÇ Model Files:\")\nif final_model.exists():\n    print(f\"  Found final model: {final_model}\")\n    eval_model_path = str(final_model)\n    model_ready = True\nelif trained_models:\n    print(f\"  Found {len(trained_models)} models\")\n    eval_model_path = trained_models[-1]\n    print(f\"  Using latest: {eval_model_path}\")\n    model_ready = True\nelse:\n    print(f\"  No trained models found\")\n    eval_model_path = None\n    model_ready = False\n\n# Verify configuration compatibility\nif model_ready:\n    config_save_path = Path('weights/training_config.json')\n    config_compatible = True\n    \n    if config_save_path.exists():\n        with open(config_save_path, 'r') as f:\n            saved_config = json.load(f)\n        \n        print(f\"\\nüîç CONFIGURATION COMPATIBILITY CHECK:\")\n        critical_params = ['out_channel', 'in_channel', 'network']\n        \n        for param in critical_params:\n            saved_value = saved_config.get(param)\n            current_value = cfg_mnet.get(param)\n            \n            if saved_value != current_value:\n                print(f\"  ‚ùå {param}: saved={saved_value}, current={current_value}\")\n                config_compatible = False\n            else:\n                print(f\"  ‚úÖ {param}: {current_value}\")\n        \n        if not config_compatible:\n            print(f\"\\n  ‚ö†Ô∏è  CRITICAL: Model/Config mismatch detected!\")\n            print(f\"  The saved model was trained with different architecture parameters.\")\n            print(f\"  This WILL cause 'size mismatch' errors during evaluation.\")\n            print(f\"\\n  Solutions:\")\n            print(f\"  1. Retrain: Set FORCE_TRAINING=True in Cell 2\")\n            print(f\"  2. Update config: Match data/config.py to saved config above\")\n            print(f\"  3. Use correct model: Place compatible model in weights/\")\n            model_ready = False\n        else:\n            print(f\"  ‚úÖ Configuration is compatible\")\n    else:\n        print(f\"\\n  ‚ö†Ô∏è  No training_config.json found\")\n        print(f\"  Cannot verify model compatibility - proceeding with caution\")\n        print(f\"  If evaluation fails with 'size mismatch', the model may be incompatible\")\n\nif model_ready and config_compatible:\n    EVAL_CONFIG = {\n        'model_path': eval_model_path,\n        'network': 'mobile0.25',\n        'confidence_threshold': 0.02,\n        'nms_threshold': 0.4,\n        'save_folder': './widerface_evaluate/widerface_txt/',\n        'dataset_folder': './data/widerface/val/images/'\n    }\n    \n    print(f\"\\nüíª Device Configuration:\")\n    print(f\"  GPU for evaluation: {'‚úÖ ENABLED' if USE_GPU_FOR_EVALUATION else '‚ùå DISABLED (CPU)'}\")\n    \n    print(f\"\\nüìä Evaluation Configuration:\")\n    for key, value in EVAL_CONFIG.items():\n        print(f\"  {key}: {value}\")\n    \n    eval_dir = Path(EVAL_CONFIG['save_folder'])\n    eval_dir.mkdir(parents=True, exist_ok=True)\n    \n    step1_cmd = [\n        'python', 'test_widerface.py',\n        '-m', EVAL_CONFIG['model_path'],\n        '--network', EVAL_CONFIG['network'],\n        '--confidence_threshold', str(EVAL_CONFIG['confidence_threshold']),\n        '--nms_threshold', str(EVAL_CONFIG['nms_threshold']),\n        '--save_folder', EVAL_CONFIG['save_folder'],\n        '--dataset_folder', EVAL_CONFIG['dataset_folder']\n    ]\n    \n    if not USE_GPU_FOR_EVALUATION or not torch.cuda.is_available():\n        step1_cmd.append('--cpu')\n    \n    step2_cmd = [\n        'cd', 'widerface_evaluate', '&&',\n        'python', 'evaluation.py',\n        '-p', EVAL_CONFIG['save_folder'],\n        '-g', './eval_tools/ground_truth'\n    ]\n    \n    print(f\"\\nüìù EVALUATION COMMANDS:\")\n    print(f\"Step 1: {' '.join(step1_cmd)}\")\n    print(f\"Step 2: {' '.join(step2_cmd)}\")\n    \n    evaluation_ready = True\nelse:\n    print(f\"\\n‚ùå Evaluation not possible\")\n    if not model_ready:\n        print(f\"  Reason: No trained model found\")\n    elif not config_compatible:\n        print(f\"  Reason: Model/Config incompatibility\")\n    evaluation_ready = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execute Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not evaluation_ready:\n",
    "    print(f\"‚ùå CANNOT EVALUATE\")\n",
    "    print(f\"   Reason: No trained model found\")\n",
    "    print(f\"   Please complete training first\")\n",
    "    evaluation_completed = False\n",
    "else:\n",
    "    print(f\"üöÄ STARTING EVALUATION\")\n",
    "    print(f\"   Device: {'GPU' if USE_GPU_FOR_EVALUATION and torch.cuda.is_available() else 'CPU'}\")\n",
    "    print(f\"   Model: {eval_model_path}\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Generate predictions\n",
    "        print(f\"üìù STEP 1: Generating predictions on validation set...\")\n",
    "        print(f\"   Command: {' '.join(step1_cmd)}\")\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        \n",
    "        result1 = subprocess.run(step1_cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result1.stdout:\n",
    "            print(result1.stdout)\n",
    "        \n",
    "        if result1.stderr:\n",
    "            print(\"\\n‚ö†Ô∏è  STDERR Output:\")\n",
    "            print(result1.stderr)\n",
    "        \n",
    "        if result1.returncode == 0:\n",
    "            print(f\"\\n{'-'*60}\")\n",
    "            print(f\"‚úÖ Step 1 completed: Predictions generated\")\n",
    "            print(f\"{'-'*60}\\n\")\n",
    "            \n",
    "            # Step 2: Calculate mAP\n",
    "            print(f\"üìù STEP 2: Calculating mAP scores...\")\n",
    "            print(f\"   Command: {' '.join(step2_cmd)}\")\n",
    "            print(f\"\\n{'-'*60}\")\n",
    "            \n",
    "            result2 = subprocess.run(' '.join(step2_cmd), shell=True, capture_output=True, text=True)\n",
    "            \n",
    "            if result2.stdout:\n",
    "                print(result2.stdout)\n",
    "            \n",
    "            if result2.stderr:\n",
    "                print(\"\\n‚ö†Ô∏è  STDERR Output:\")\n",
    "                print(result2.stderr)\n",
    "            \n",
    "            if result2.returncode == 0:\n",
    "                print(f\"\\n{'-'*60}\")\n",
    "                print(f\"‚úÖ Step 2 completed: mAP calculated\")\n",
    "                print(f\"{'-'*60}\")\n",
    "                evaluation_completed = True\n",
    "            else:\n",
    "                print(f\"\\n‚ùå Step 2 failed (exit code: {result2.returncode})\")\n",
    "                evaluation_completed = False\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Step 1 failed (exit code: {result1.returncode})\")\n",
    "            print(f\"   Cannot proceed to Step 2\")\n",
    "            evaluation_completed = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚ùå EVALUATION ERROR: {e}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        evaluation_completed = False\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä EVALUATION STATUS: {'‚úÖ COMPLETED' if evaluation_completed else '‚ùå FAILED/PENDING'}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if evaluation_completed:\n",
    "    print(f\"\\nüìÅ Evaluation results:\")\n",
    "    print(f\"   ‚Ä¢ Predictions: {EVAL_CONFIG['save_folder']}\")\n",
    "    print(f\"   ‚Ä¢ mAP scores: See output above\")\n",
    "    print(f\"   ‚Ä¢ Check for Easy/Medium/Hard mAP values\")\n",
    "else:\n",
    "    print(f\"\\nüí° Review errors above to troubleshoot evaluation issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üì¶ MODEL EXPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "export_device = 'gpu' if USE_GPU_FOR_EXPORT and torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üíª Export Device: {export_device.upper()}\")\n",
    "\n",
    "model_available_for_export = ('model_ready' in locals() and model_ready) or final_model.exists()\n",
    "\n",
    "if model_available_for_export:\n",
    "    export_dir = Path('exports')\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    exports = {\n",
    "        'pytorch': export_dir / 'featherface_model.pth',\n",
    "        'onnx': export_dir / 'featherface_model.onnx',\n",
    "        'torchscript': export_dir / 'featherface_model.pt'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìÇ Export directory: {export_dir}\")\n",
    "    print(f\"Available formats: {', '.join(exports.keys())}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nüì• Loading model...\")\n",
    "        export_model = RetinaFace(cfg=cfg_mnet, phase='test')\n",
    "        \n",
    "        if 'eval_model_path' in locals() and eval_model_path:\n",
    "            state_dict = torch.load(eval_model_path, map_location='cpu')\n",
    "            \n",
    "            # Handle different state dict formats\n",
    "            if 'state_dict' in state_dict:\n",
    "                state_dict = state_dict['state_dict']\n",
    "            \n",
    "            # Remove 'module.' prefix if present\n",
    "            from collections import OrderedDict\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k.replace('module.', '') if k.startswith('module.') else k\n",
    "                new_state_dict[name] = v\n",
    "            \n",
    "            export_model.load_state_dict(new_state_dict, strict=False)\n",
    "            print(f\"‚úÖ Loaded weights from {eval_model_path}\")\n",
    "        \n",
    "        export_model.eval()\n",
    "        \n",
    "        export_params = sum(p.numel() for p in export_model.parameters())\n",
    "        print(f\"\\nüìä Model Info:\")\n",
    "        print(f\"  Parameters: {export_params:,} ({export_params/1e6:.3f}M)\")\n",
    "        print(f\"  Architecture: RetinaFace with CBAM\")\n",
    "        print(f\"  Input shape: [batch, 3, 640, 640]\")\n",
    "        print(f\"  Export device: {export_device.upper()}\")\n",
    "        \n",
    "        exported_files = {}\n",
    "        \n",
    "        # 1. Export PyTorch format (always on CPU for compatibility)\n",
    "        print(f\"\\nüì¶ Exporting formats...\")\n",
    "        print(f\"  1. PyTorch (.pth)...\")\n",
    "        torch.save(export_model.cpu().state_dict(), exports['pytorch'])\n",
    "        exported_files['pytorch'] = exports['pytorch']\n",
    "        print(f\"     ‚úÖ Saved: {exports['pytorch']}\")\n",
    "        \n",
    "        # 2. Export ONNX format\n",
    "        try:\n",
    "            import onnx\n",
    "            import onnxruntime\n",
    "            \n",
    "            print(f\"  2. ONNX (.onnx)...\")\n",
    "            print(f\"     ONNX version: {onnx.__version__}\")\n",
    "            \n",
    "            # Prepare model for ONNX export (CPU)\n",
    "            export_model_cpu = export_model.cpu()\n",
    "            export_model_cpu.eval()\n",
    "            \n",
    "            # Create dummy input\n",
    "            batch_size = 1\n",
    "            img_size = 640\n",
    "            dummy_input = torch.randn(batch_size, 3, img_size, img_size)\n",
    "            \n",
    "            # Test forward pass\n",
    "            with torch.no_grad():\n",
    "                torch_output = export_model_cpu(dummy_input)\n",
    "            \n",
    "            print(f\"     PyTorch output shapes: {[out.shape for out in torch_output]}\")\n",
    "            \n",
    "            # Export to ONNX\n",
    "            input_names = ['input']\n",
    "            output_names = ['bbox', 'conf', 'landmarks']\n",
    "            \n",
    "            torch.onnx.export(\n",
    "                export_model_cpu,\n",
    "                dummy_input,\n",
    "                str(exports['onnx']),\n",
    "                export_params=True,\n",
    "                opset_version=12,\n",
    "                do_constant_folding=True,\n",
    "                input_names=input_names,\n",
    "                output_names=output_names,\n",
    "                dynamic_axes={\n",
    "                    'input': {0: 'batch'},\n",
    "                    'bbox': {0: 'batch'},\n",
    "                    'conf': {0: 'batch'},\n",
    "                    'landmarks': {0: 'batch'}\n",
    "                },\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Verify ONNX model\n",
    "            onnx_model = onnx.load(str(exports['onnx']))\n",
    "            onnx.checker.check_model(onnx_model)\n",
    "            print(f\"     ‚úÖ ONNX model validated\")\n",
    "            \n",
    "            # Test ONNX inference\n",
    "            print(f\"     Testing ONNX inference...\")\n",
    "            providers = ['CPUExecutionProvider']\n",
    "            session = onnxruntime.InferenceSession(str(exports['onnx']), providers=providers)\n",
    "            \n",
    "            # Run inference\n",
    "            onnx_input = {session.get_inputs()[0].name: dummy_input.numpy()}\n",
    "            onnx_outputs = session.run(None, onnx_input)\n",
    "            \n",
    "            print(f\"     ONNX output shapes: {[out.shape for out in onnx_outputs]}\")\n",
    "            \n",
    "            # Compare outputs\n",
    "            max_diff = max([abs(torch_output[i].cpu().numpy() - onnx_outputs[i]).max() \n",
    "                           for i in range(len(onnx_outputs))])\n",
    "            print(f\"     Max difference (PyTorch vs ONNX): {max_diff:.6f}\")\n",
    "            \n",
    "            if max_diff < 1e-3:\n",
    "                print(f\"     ‚úÖ ONNX inference matches PyTorch (diff < 1e-3)\")\n",
    "            else:\n",
    "                print(f\"     ‚ö†Ô∏è  ONNX inference differs from PyTorch (diff = {max_diff:.6f})\")\n",
    "            \n",
    "            exported_files['onnx'] = exports['onnx']\n",
    "            print(f\"     ‚úÖ Saved: {exports['onnx']}\")\n",
    "            \n",
    "        except ImportError as e:\n",
    "            print(f\"     ‚ö†Ô∏è  ONNX export skipped: {e}\")\n",
    "            print(f\"     Install with: pip install onnx onnxruntime\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ö†Ô∏è  ONNX export failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # 3. Export TorchScript format\n",
    "        try:\n",
    "            print(f\"  3. TorchScript (.pt)...\")\n",
    "            \n",
    "            # TorchScript export on CPU\n",
    "            export_model_cpu = export_model.cpu()\n",
    "            export_model_cpu.eval()\n",
    "            dummy_input_ts = torch.randn(1, 3, 640, 640)\n",
    "            \n",
    "            traced_model = torch.jit.trace(export_model_cpu, dummy_input_ts)\n",
    "            traced_model.save(str(exports['torchscript']))\n",
    "            \n",
    "            # Test TorchScript\n",
    "            loaded_ts = torch.jit.load(str(exports['torchscript']))\n",
    "            with torch.no_grad():\n",
    "                ts_output = loaded_ts(dummy_input_ts)\n",
    "            print(f\"     TorchScript output shapes: {[out.shape for out in ts_output]}\")\n",
    "            \n",
    "            exported_files['torchscript'] = exports['torchscript']\n",
    "            print(f\"     ‚úÖ Saved: {exports['torchscript']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ö†Ô∏è  TorchScript export failed: {e}\")\n",
    "        \n",
    "        # File sizes\n",
    "        print(f\"\\nüì¶ Exported Files:\")\n",
    "        for format_name, file_path in exported_files.items():\n",
    "            if file_path.exists():\n",
    "                file_size = file_path.stat().st_size / (1024 * 1024)\n",
    "                print(f\"  ‚Ä¢ {format_name.upper()}: {file_path.name} ({file_size:.2f} MB)\")\n",
    "        \n",
    "        # Usage examples\n",
    "        print(f\"\\nüìù Usage Examples:\")\n",
    "        print(f\"  # PyTorch\")\n",
    "        print(f\"  from models.retinaface import RetinaFace\")\n",
    "        print(f\"  from data.config import cfg_mnet\")\n",
    "        print(f\"  model = RetinaFace(cfg=cfg_mnet, phase='test')\")\n",
    "        print(f\"  model.load_state_dict(torch.load('{exports['pytorch']}'))\")\n",
    "        \n",
    "        if 'onnx' in exported_files:\n",
    "            print(f\"\\n  # ONNX Runtime\")\n",
    "            print(f\"  import onnxruntime as ort\")\n",
    "            print(f\"  session = ort.InferenceSession('{exports['onnx']}')\")\n",
    "            print(f\"  outputs = session.run(None, {{'input': img_tensor.numpy()}})\")\n",
    "        \n",
    "        if 'torchscript' in exported_files:\n",
    "            print(f\"\\n  # TorchScript\")\n",
    "            print(f\"  model = torch.jit.load('{exports['torchscript']}')\")\n",
    "            print(f\"  outputs = model(img_tensor)\")\n",
    "        \n",
    "        export_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Export failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        export_success = False\n",
    "else:\n",
    "    print(f\"‚ùå No trained model available\")\n",
    "    export_success = False\n",
    "\n",
    "print(f\"\\nStatus: {'‚úÖ READY FOR DEPLOYMENT' if export_success else '‚ö†Ô∏è TRAIN FIRST'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìä PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "completion_status = {\n",
    "    'Environment Setup': True,\n",
    "    'Model Validation': overall_valid if 'overall_valid' in locals() else False,\n",
    "    'Dataset Management': overall_ready if 'overall_ready' in locals() else False,\n",
    "    'Training Pipeline': all_ready if 'all_ready' in locals() else False,\n",
    "    'Evaluation System': evaluation_ready if 'evaluation_ready' in locals() else False,\n",
    "    'Model Export': export_success if 'export_success' in locals() else False\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Status:\")\n",
    "for component, status in completion_status.items():\n",
    "    print(f\"  {component}: {'‚úÖ' if status else '‚ùå'}\")\n",
    "\n",
    "overall_completion = sum(completion_status.values()) / len(completion_status)\n",
    "print(f\"\\nCompletion: {overall_completion*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüíª Device Configuration Summary:\")\n",
    "print(f\"  Training: {'GPU' if USE_GPU_FOR_TRAINING and torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"  Evaluation: {'GPU' if USE_GPU_FOR_EVALUATION and torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"  Export: {'GPU' if USE_GPU_FOR_EXPORT and torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "print(f\"  Source: data/config.py\")\n",
    "print(f\"  Config: cfg_mnet\")\n",
    "print(f\"  Network: {cfg_mnet['name']}\")\n",
    "print(f\"  Architecture: RetinaFace with CBAM\")\n",
    "\n",
    "print(f\"\\nüìú Scripts:\")\n",
    "print(f\"  Training: train.py\")\n",
    "print(f\"  Testing: test_widerface.py\")\n",
    "print(f\"  Evaluation: widerface_evaluate/evaluation.py\")\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"\\nüìÖ {current_time}\")\n",
    "print(f\"üíª PyTorch {torch.__version__}\")\n",
    "\n",
    "print(f\"\\nüí° CONFIGURATION TIPS:\")\n",
    "print(f\"  ‚Ä¢ To use CPU for evaluation: Set USE_GPU_FOR_EVALUATION = False in Cell 2\")\n",
    "print(f\"  ‚Ä¢ To skip training: Set SKIP_TRAINING = True in Cell 2\")\n",
    "print(f\"  ‚Ä¢ To force training: Set FORCE_TRAINING = True in Cell 2\")\n",
    "print(f\"  ‚Ä¢ To change model path: Edit TRAINED_MODEL_PATH in Cell 2\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ NOTEBOOK READY\")\n",
    "print(\"üîß Flexible GPU/CPU configuration enabled\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}